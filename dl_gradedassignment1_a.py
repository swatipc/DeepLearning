# -*- coding: utf-8 -*-
"""DL_GradedAssignment1_A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18makGMoCQmh5UWAEOgGo4uLh-T-blZd4
"""

#Importing required libraries
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np
import numpy.random as rnd
from sklearn.metrics import accuracy_score
from numpy import random

# Loading MNIST dataset provided by sklearn

MNIST = load_digits()
print("Shape of MNIST dataset:", MNIST.data.shape)

# Plotting image 
plt.matshow(MNIST.images[9])

# Normalizing data
xScale = StandardScaler()
X = xScale.fit_transform(MNIST.data)

# Splitting data into Test and Train
Y = MNIST.target
xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.4)

print(yTrain)

# Converting Y values to vector
def yConversion(Y):
    yVector = np.zeros((len(Y), 10))
    for i in range(len(Y)):
        yVector[i, Y[i]] = 1
    return yVector

yTrain_new = yConversion(yTrain)
yTest_new = yConversion(yTest)

print("Updated yTrain:", yTrain_new[0])
print("Updated yTest:", yTest_new[0])

print(xTrain.shape)
print(yTrain_new[0].shape)
print(xTest.shape)
print(yTest_new[0].shape)

#Defining number of nodes in input layer, hidden layer and output layer repsectively
num_nodes = [64, 30, 10]

#Activation function

def sigmoid(x):
    return 1 / (1 + np.exp(-x))
    
def sigmoid_deriv(x):
    return sigmoid(x) * (1 - sigmoid(x))

#Weight initialization
def initWeights(num_nodes):
    w = {}
    b = {}
    for i in range(1, len(num_nodes)):
        w[i] = rnd.random_sample((num_nodes[i], num_nodes[i-1]))
        b[i] = rnd.random_sample((num_nodes[i],))
    return w, b

#calculating delta of weight and bias
def initDelta(num_nodes):
    dw = {}
    db = {}
    for i in range(1, len(num_nodes)):
        dw[i] = np.zeros((num_nodes[i], num_nodes[i-1]))
        db[i] = np.zeros((num_nodes[i],))
    return dw, db

# Forward Propagation
def feedForward(x, w, b):
    h = {1: x}
    z = {}
    for i in range(1, len(w) + 1):
        if i == 1:
            node_ip = x
        else:
            node_ip = h[i]
        z[i+1] = w[i].dot(node_ip) + b[i] 
        h[i+1] = sigmoid(z[i+1]) 
    return h, z

# Calculating delta of output of hidden layer and output layer
 
def calOutDelta(y, h_out, z_out):
    return -(y - h_out) * sigmoid_deriv(z_out)

def calHiddenDelta(delta_1, w_i, z_i):
    return np.dot(np.transpose(w_i), delta_1) * sigmoid_deriv(z_i)

# Dividing input data into mini batches

def miniBatches(x, y, batchSize):
    rand_n = random.choice(len(y), len(y), replace=False)
    #Shuffle X and Y
    shuffledX = x[rand_n,:]
    shuffledY = y[rand_n]
    for i in range(0,len(y),batchSize):
      miniBatch = (shuffledX[i:i+batchSize,:], shuffledY[i:i+batchSize])

    return miniBatch

def train_MBGD(num_nodes, x, y, batchSize=100, iterations=3000, alpha=0.25, lamb=0.000):
    w, b = initWeights(num_nodes)
    count = 0
    m = len(y)
    costFunc = []
    while count < iterations:
        dw, db = initDelta(num_nodes)
        avgCost = 0
        miniBatch = miniBatches(x, y, batchSize)
        for batch in miniBatch:
            x_mb = batch[0]
            y_mb = batch[1]
            for n in range(0,len(y_mb)):
                d = {}
                # Forward Propagation
                h, z = feedForward(x_mb[n, :], w, b)
               
                # Backward propagation
                for i in range(len(num_nodes), 0, -1):
                    if i == len(num_nodes):
                        d[i] = calOutDelta(y_mb[i,:], h[i], z[i])
                        avgCost += np.linalg.norm((y_mb[i,:]-h[i]))
                    else:
                        if i > 1:
                            d[i] = calHiddenDelta(d[i+1], w[i], z[i])
                        dw[i] += np.dot(d[i+1][:,np.newaxis], np.transpose(h[i][:,np.newaxis])) 
                        db[i] += d[i+1]
            # Gradient Descent
            for n in range(len(num_nodes) - 1, 0, -1):
                w[n] += -alpha * (1.0/batchSize * dw[n] + lamb * w[n])
                b[n] += -alpha * (1.0/batchSize * db[n])
        # calculating cost function
        avgCost = 1.0/m * avgCost
        costFunc.append(avgCost)
        count += 1
    return w, b, costFunc

w, b, avgCostFunc =train_MBGD(num_nodes, xTrain, yTrain_new, batchSize=100, iterations=3000, alpha=0.25, lamb=0.000)

# Predicting digit
def predictNum(w, b, x, num_layers):
    m = x.shape[0]
    y = np.zeros((m,))
    for n in range(m):
        h, z = feedForward(x[n, :], w, b)
        y[n] = np.argmax(h[num_layers])
    return y

#Accuracy
yPred = predictNum(w, b, xTest, 3)
accuracy_score(yTest, yPred)*100

